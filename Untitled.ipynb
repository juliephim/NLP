{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2008591d-dea5-43b4-84c0-9d0e2362e062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\phimj\\python\\python311\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\phimj\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\phimj\\python\\python311\\lib\\site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\phimj\\python\\python311\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\phimj\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\phimj\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\phimj\\python\\python311\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\phimj\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\phimj\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.0 MB 487.6 kB/s eta 0:00:50\n",
      "   ---------------------------------------- 0.2/24.0 MB 1.3 MB/s eta 0:00:19\n",
      "    --------------------------------------- 0.5/24.0 MB 3.0 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.6/24.0 MB 3.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.0/24.0 MB 3.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 1.2/24.0 MB 3.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.5/24.0 MB 4.0 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.5/24.0 MB 4.1 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.7/24.0 MB 3.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.8/24.0 MB 3.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.0/24.0 MB 3.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.4/24.0 MB 4.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.6/24.0 MB 4.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/24.0 MB 4.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 4.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.6/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.6/24.0 MB 4.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.1/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.3/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.3/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.4/24.0 MB 4.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.8/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.0/24.0 MB 4.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.4/24.0 MB 4.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 5.7/24.0 MB 4.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.0/24.0 MB 4.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.3/24.0 MB 4.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 6.7/24.0 MB 4.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.1/24.0 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.4/24.0 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 7.8/24.0 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.0/24.0 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.4/24.0 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.6/24.0 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.0/24.0 MB 5.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.2/24.0 MB 5.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.6/24.0 MB 5.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 9.9/24.0 MB 5.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.0/24.0 MB 5.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.5/24.0 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.7/24.0 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.2/24.0 MB 5.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.3/24.0 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.6/24.0 MB 5.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.9/24.0 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.3/24.0 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.5/24.0 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.9/24.0 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.2/24.0 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.5/24.0 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 13.9/24.0 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.2/24.0 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 14.6/24.0 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 14.8/24.0 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.2/24.0 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.4/24.0 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 15.9/24.0 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.0/24.0 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.4/24.0 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.7/24.0 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.2/24.0 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.4/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.8/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.2/24.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.4/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.9/24.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.1/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.5/24.0 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.7/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.2/24.0 MB 6.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.8/24.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.0/24.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.5/24.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.7/24.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.2/24.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.5/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.7/24.0 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.0/24.0 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.4/24.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 6.1 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming 'processed_docs' is a list of lists of words, e.g., [['first', 'document'], ['second', 'document']]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# You can create 'processed_docs' by preprocessing the NFCorpus data as needed.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(\u001b[43mprocessed_docs\u001b[49m, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Clone the NFCorpus data repository\u001b[39;00m\n\u001b[0;32m     12\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgit clone https://github.com/cr-nlp/project1-2023.git\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install gensim numpy nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming 'processed_docs' is a list of lists of words, e.g., [['first', 'document'], ['second', 'document']]\n",
    "# You can create 'processed_docs' by preprocessing the NFCorpus data as needed.\n",
    "model = Word2Vec(processed_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# Clone the NFCorpus data repository\n",
    "!git clone https://github.com/cr-nlp/project1-2023.git\n",
    "\n",
    "# Import necessary libraries\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93439da0-e575-4c56-9d20-c2dccdd5a493",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dicDoc, dicReq, dicReqDoc\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Define stopwords outside the function to avoid repeated loading\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Tokenize and remove stopwords\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a function to load the NFCorpus data from the cloned GitHub repository.\n",
    "def loadNFCorpus():\n",
    "    # Define the directory where the data is located.\n",
    "    dir = \"./project1-2023/\"\n",
    "    # Load the document data which contains abstracts from PubMed.\n",
    "    filename = dir + \"dev.docs\"\n",
    "    \n",
    "    # Initialize a dictionary to store document data.\n",
    "    dicDoc = {}\n",
    "    # Read document lines and split them into a dictionary with key as document ID and value as text.\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        tabLine = line.split('\\t')\n",
    "        key = tabLine[0]\n",
    "        value = tabLine[1]\n",
    "        dicDoc[key] = value\n",
    "    \n",
    "    # Load and parse the query data similar to document data.\n",
    "    filename = dir + \"dev.all.queries\"\n",
    "    dicReq = {}\n",
    "    with open(filename, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        tabLine = line.split('\\t')\n",
    "        key = tabLine[0]\n",
    "        value = tabLine[1]\n",
    "        dicReq[key] = value\n",
    "    \n",
    "    # Load the relevance judgments which provide a relevance score for document-query pairs.\n",
    "    filename = dir + \"dev.2-1-0.qrel\"\n",
    "    dicReqDoc = defaultdict(dict)\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        tabLine = line.strip().split('\\t')\n",
    "        req = tabLine[0]\n",
    "        doc = tabLine[2]\n",
    "        score = int(tabLine[3])\n",
    "        dicReqDoc[req][doc] = score\n",
    "    \n",
    "    # Return the loaded document and query data along with relevance judgments.\n",
    "    return dicDoc, dicReq, dicReqDoc\n",
    "\n",
    "# Define stopwords outside the function to avoid repeated loading\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2 and word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "def text_to_embedding(text, model):\n",
    "    # Preprocess and tokenize text\n",
    "    tokens = preprocess_text(text)\n",
    "    # Filter tokens based on the model's vocabulary and get embeddings\n",
    "    embeddings = np.array([model[word] for word in tokens if word in model])\n",
    "    \n",
    "    # If at least one token produced an embedding, return the mean vector\n",
    "    if len(embeddings) > 0: \n",
    "        return np.mean(embeddings, axis=0)\n",
    "    # If no valid embeddings were retrieved (i.e., text had no known tokens), return a zero vector\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'model' is your loaded Word2Vec or BioWordVec model\n",
    "text = \"Example text to be vectorized using word embeddings.\"\n",
    "vector = text_to_embedding(text, model)\n",
    "print(vector)  # This will print the averaged word vector for the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe4e17-f4ab-4316-a9ef-7d2930506080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query text\n",
    "user_query = \"What are the latest treatments for diabetes mellitus?\"\n",
    "\n",
    "# Convert user query to an embedding using the previously defined function\n",
    "# Let's assume 'word_vectors' is already loaded with the BioWordVec model.\n",
    "query_embedding = text_to_embedding(user_query, word_vectors)\n",
    "\n",
    "# Assuming 'dicDoc' contains document IDs mapped to their content, let's calculate the cosine similarity\n",
    "# for each document with respect to the query.\n",
    "# First, we convert all documents to embeddings (this would be done once and cached in practice):\n",
    "document_embeddings = {doc_id: text_to_embedding(doc_content, word_vectors) for doc_id, doc_content in dicDoc.items()}\n",
    "\n",
    "# Now calculate the similarity of the query to each document\n",
    "doc_similarity_scores = {doc_id: cosine_similarity(query_embedding, doc_embedding)\n",
    "                         for doc_id, doc_embedding in document_embeddings.items() if np.any(doc_embedding)}\n",
    "\n",
    "# Sort the documents based on their similarity scores in descending order\n",
    "sorted_doc_ids = sorted(doc_similarity_scores, key=doc_similarity_scores.get, reverse=True)\n",
    "\n",
    "# Display the top 5 most similar documents along with their titles and a snippet of their content\n",
    "print(\"Top 5 relevant documents for the query:\")\n",
    "for doc_id in sorted_doc_ids[:5]:\n",
    "    print(f\"Doc ID: {doc_id}\")\n",
    "    print(f\"Title: {dicDoc[doc_id].split('.')[0]}\")  # Assuming the title ends with a period.\n",
    "    print(f\"Content Snippet: {' '.join(dicDoc[doc_id].split(' ')[:50])}...\")  # Display first 50 words\n",
    "    print(f\"Similarity Score: {doc_similarity_scores[doc_id]:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
